{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.load import MetaLoader, DialogueTrainLoader\n",
    "from utils.preprocess import Preprocessor, Augmentation\n",
    "from utils.encoder import Encoder\n",
    "\n",
    "from data import FH2024Dataset, collate_fn\n",
    "from net.tokenizer import SubWordEmbReaderUtil\n",
    "from net.model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Initialize subword embedding>\n",
      "loading= ./sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat\n"
     ]
    }
   ],
   "source": [
    "swer_path = './sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat'\n",
    "swer = SubWordEmbReaderUtil(swer_path)\n",
    "\n",
    "meta_path = '../../datasets/FH_2024/subtask3/mdata.wst.txt.2023.08.23'\n",
    "meta_loader = MetaLoader(path=meta_path, swer=swer)\n",
    "img2id, id2img, img_similarity = meta_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../../datasets/FH_2024/subtask3/task1.ddata.wst.txt\"\n",
    "train_diag_loader = DialogueTrainLoader(path=train_path)\n",
    "train_raw_dataset = train_diag_loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(num_rank=3, num_coordi=4, threshold=0.7)\n",
    "train_dataset = preprocessor(train_raw_dataset, img2id, id2img, img_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = Augmentation(num_aug=5, num_rank=3, num_coordi=4, threshold=0.7)\n",
    "train_dataset = augmentation(train_dataset, img2id, id2img, img_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(swer=swer, img2id=img2id, num_coordi=4, mem_size=16, meta_size=4)\n",
    "encoded_train_dataset = encoder(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -- Training\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 10\n",
    "\n",
    "# -- Model\n",
    "KEY_SIZE = 300\n",
    "MEM_SIZE = 16\n",
    "HOPS = 3\n",
    "EVAL_NODE = '[6000,6000,6000,200][2000,2000]'\n",
    "DROP_PROB = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FH2024Dataset(dataset=encoded_train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_size = [len(img2id[i]) for i in range(4)] # 각 카테고리별 개수\n",
    "net = Model(emb_size=swer.get_emb_size(), \n",
    "            key_size=KEY_SIZE, \n",
    "            mem_size=MEM_SIZE,\n",
    "            meta_size=4, \n",
    "            hops=HOPS, \n",
    "            item_size=item_size, \n",
    "            coordi_size=4,\n",
    "            eval_node=EVAL_NODE, \n",
    "            num_rnk=3, \n",
    "            use_batch_norm=False, \n",
    "            use_dropout=True,\n",
    "            zero_prob=DROP_PROB,\n",
    "            use_multimodal=False,\n",
    "            img_feat_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ho/workspace/FH_2024/subtask3/data.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coordi_tensor = torch.tensor(coordi_tensor, dtype=torch.long)\n",
      "/home/ho/workspace/FH_2024/subtask3/data.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coordi_tensor = torch.tensor(coordi_tensor, dtype=torch.long)\n",
      "/home/ho/workspace/FH_2024/subtask3/data.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coordi_tensor = torch.tensor(coordi_tensor, dtype=torch.long)\n",
      "/home/ho/workspace/FH_2024/subtask3/data.py:40: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  coordi_tensor = torch.tensor(coordi_tensor, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "# 필요한 추가 임포트\n",
    "import torch\n",
    "\n",
    "# train_loader에서 하나의 배치만 가져오기\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "# GPU 사용 가능시 모델을 GPU로 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = net.to(device)\n",
    "\n",
    "# 배치의 첫 번째 샘플 선택\n",
    "desc = batch['description'][0].unsqueeze(0).to(device)  # shape: (1, mem_size, emb_size)\n",
    "coordi = batch['coordi'][0].unsqueeze(0).to(device)  # shape: (1, num_rank, emb_size * coordi_size)\n",
    "actual_rank = batch['rank'][0].item()\n",
    "\n",
    "# 모델 실행\n",
    "net.eval()  # 평가 모드 설정\n",
    "with torch.no_grad():\n",
    "    logits = net(desc, coordi)\n",
    "    preds = torch.argsort(logits, -1, descending=True).cpu().numpy()[0]\n",
    "\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_weighted_kendall_tau(pred, label):\n",
    "    total_count = 0\n",
    "    total_corr = 0.0\n",
    "\n",
    "    for p, l in zip(pred, label):\n",
    "        corr, _ = stats.weightedtau(p, l)\n",
    "        total_corr += corr\n",
    "        total_count += 1\n",
    "    \n",
    "    return (total_corr / total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WKT:  0.5455\n"
     ]
    }
   ],
   "source": [
    "label = [[0, 1, 2], [0, 2, 1], [1, 0, 2], [1, 2, 0]]\n",
    "pred = [[1, 0, 2], [1, 2, 0], [0, 1, 2], [0, 2, 1]]\n",
    "\n",
    "corr = _calculate_weighted_kendall_tau(pred, label)\n",
    "print(f\"WKT: {corr: .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
